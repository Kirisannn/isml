{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.loadtxt(\"train.csv\", delimiter=\",\")\n",
    "testing_data = np.loadtxt(\"test.csv\", delimiter=\",\")\n",
    "\n",
    "# Split training data\n",
    "X = training_data[:, 1:]\n",
    "y = training_data[:, 0]\n",
    "y[y == 0] = -1 # Convert labels from {0,1} to {-1,1}\n",
    "X_train = X[:4000] # Use the first 4000 samples as training data\n",
    "y_train = y[:4000]\n",
    "X_val = X[4000:] # Remaining samples as validation data\n",
    "y_val = y[4000:]\n",
    "\n",
    "# Split testing data\n",
    "X_test = testing_data[:, 1:]\n",
    "y_test = testing_data[:, 0]\n",
    "y_test[y_test == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_val = scaler.transform(X_val)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the trained SVM model parameters using primal form\n",
    "def svm_train_primal(data_train, label_train, regularisation_para_C):\n",
    "    num_samples, num_features = data_train.shape\n",
    "\n",
    "    w = cp.Variable(num_features, value=np.zeros(num_features)) # Weight vector\n",
    "    b = cp.Variable(value=0.0) # Bias term\n",
    "    xi = cp.Variable(num_samples) # Slack variables\n",
    "\n",
    "    # Define objective function\n",
    "    objective = cp.Minimize(0.5 * cp.norm(w,2)**2 + regularisation_para_C * cp.sum(xi) / num_samples)\n",
    "\n",
    "    # Define constraints\n",
    "    constraints = [label_train[i] * (data_train[i] @ w + b) >= 1 - xi[i] for i in range(num_samples)]\n",
    "    constraints += [xi >= 0]\n",
    "    \n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve(verbose=True)\n",
    "\n",
    "    # Return SVM model parameters\n",
    "    svm_primal_model = {\"w\": w.value, \"b\": b.value, \"xi\": xi.value}\n",
    "    return svm_primal_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict on testing data and calculate accuracy\n",
    "def svm_predict_primal(data_test, label_test, svm_primal_model):\n",
    "    w = svm_primal_model[\"w\"]\n",
    "    b = svm_primal_model[\"b\"]\n",
    "\n",
    "    pred = np.sign(data_test @ w + b)\n",
    "    accuracy = np.mean(pred == label_test)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================\n",
      "                                     CVXPY                                     \n",
      "                                     v1.5.3                                    \n",
      "===============================================================================\n",
      "(CVXPY) Sep 10 01:46:44 PM: Your problem has 4201 variables, 8000 constraints, and 0 parameters.\n",
      "(CVXPY) Sep 10 01:46:45 PM: It is compliant with the following grammars: DCP, DQCP\n",
      "(CVXPY) Sep 10 01:46:45 PM: (If you need to solve this problem multiple times, but with different data, consider using parameters.)\n",
      "(CVXPY) Sep 10 01:46:45 PM: CVXPY will first compile your problem; then, it will invoke a numerical solver to obtain a solution.\n",
      "(CVXPY) Sep 10 01:46:45 PM: Your problem is compiled with the CPP canonicalization backend.\n",
      "-------------------------------------------------------------------------------\n",
      "                                  Compilation                                  \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Sep 10 01:46:45 PM: Compiling problem (target solver=CLARABEL).\n",
      "(CVXPY) Sep 10 01:46:45 PM: Reduction chain: Dcp2Cone -> CvxAttr2Constr -> ConeMatrixStuffing -> CLARABEL\n",
      "(CVXPY) Sep 10 01:46:45 PM: Applying reduction Dcp2Cone\n",
      "(CVXPY) Sep 10 01:46:45 PM: Applying reduction CvxAttr2Constr\n",
      "(CVXPY) Sep 10 01:46:45 PM: Applying reduction ConeMatrixStuffing\n",
      "(CVXPY) Sep 10 01:47:01 PM: Applying reduction CLARABEL\n",
      "(CVXPY) Sep 10 01:47:01 PM: Finished problem compilation (took 1.625e+01 seconds).\n",
      "-------------------------------------------------------------------------------\n",
      "                                Numerical solver                               \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Sep 10 01:47:01 PM: Invoking solver CLARABEL  to obtain a solution.\n",
      "-------------------------------------------------------------\n",
      "           Clarabel.rs v0.9.0  -  Clever Acronym                \n",
      "\n",
      "                   (c) Paul Goulart                          \n",
      "                University of Oxford, 2022                   \n",
      "-------------------------------------------------------------\n",
      "\n",
      "problem:\n",
      "  variables     = 4202\n",
      "  constraints   = 8201\n",
      "  nnz(P)        = 1\n",
      "  nnz(A)        = 809221\n",
      "  cones (total) = 2\n",
      "    : Nonnegative = 1,  numel = 8000\n",
      "    : SecondOrder = 1,  numel = 201\n",
      "\n",
      "settings:\n",
      "  linear algebra: direct / qdldl, precision: 64 bit\n",
      "  max iter = 200, time limit = Inf,  max step = 0.990\n",
      "  tol_feas = 1.0e-8, tol_gap_abs = 1.0e-8, tol_gap_rel = 1.0e-8,\n",
      "  static reg : on, ϵ1 = 1.0e-8, ϵ2 = 4.9e-32\n",
      "  dynamic reg: on, ϵ = 1.0e-13, δ = 2.0e-7\n",
      "  iter refine: on, reltol = 1.0e-13, abstol = 1.0e-12,\n",
      "               max iter = 10, stop ratio = 5.0\n",
      "  equilibrate: on, min_scale = 1.0e-4, max_scale = 1.0e4\n",
      "               max iter = 10\n",
      "\n",
      "iter    pcost        dcost       gap       pres      dres      k/t        μ       step      \n",
      "---------------------------------------------------------------------------------------------\n",
      "  0  -9.1129e+01  +1.4340e+02  2.57e+00  9.44e-01  3.97e+01  1.00e+00  6.52e+00   ------   \n",
      "  1  -3.4854e+01  +7.0667e+01  3.03e+00  7.27e-01  3.30e+01  1.29e+00  4.64e+00  4.86e-01  \n",
      "  2  -2.3213e+01  +5.1501e+01  3.22e+00  4.88e-01  2.47e+01  2.69e+00  3.60e+00  8.31e-01  \n",
      "  3  +2.3626e+00  +1.6154e+01  5.84e+00  1.09e-01  6.37e+00  1.19e+00  9.38e-01  7.98e-01  \n",
      "  4  +5.2079e+00  +9.9481e+00  9.10e-01  3.48e-02  2.23e+00  4.02e-01  3.28e-01  6.88e-01  \n",
      "  5  +5.8874e+00  +7.9079e+00  3.43e-01  1.42e-02  9.48e-01  1.51e-01  1.39e-01  6.42e-01  \n",
      "  6  +6.0418e+00  +7.5265e+00  2.46e-01  1.04e-02  6.97e-01  7.66e-02  1.03e-01  5.31e-01  \n",
      "  7  +6.2486e+00  +6.7317e+00  7.73e-02  3.33e-03  2.26e-01  2.27e-02  3.34e-02  7.18e-01  \n",
      "  8  +6.3172e+00  +6.4948e+00  2.81e-02  1.22e-03  8.28e-02  4.01e-03  1.23e-02  8.83e-01  \n",
      "  9  +6.3337e+00  +6.4105e+00  1.21e-02  5.28e-04  3.57e-02  1.64e-03  5.32e-03  6.17e-01  \n",
      " 10  +6.3393e+00  +6.3874e+00  7.59e-03  3.31e-04  2.24e-02  7.84e-04  3.34e-03  7.04e-01  \n",
      " 11  +6.3437e+00  +6.3622e+00  2.92e-03  1.27e-04  8.61e-03  2.81e-04  1.28e-03  7.05e-01  \n",
      " 12  +6.3454e+00  +6.3524e+00  1.10e-03  4.77e-05  3.23e-03  9.69e-05  4.82e-04  7.60e-01  \n",
      " 13  +6.3461e+00  +6.3482e+00  3.33e-04  1.45e-05  9.83e-04  2.84e-05  1.46e-04  7.67e-01  \n",
      " 14  +6.3464e+00  +6.3468e+00  7.11e-05  3.10e-06  2.10e-04  5.65e-06  3.13e-05  8.99e-01  \n",
      " 15  +6.3464e+00  +6.3465e+00  1.33e-05  5.79e-07  3.92e-05  1.05e-06  5.85e-06  8.39e-01  \n",
      " 16  +6.3464e+00  +6.3465e+00  1.25e-06  5.46e-08  3.70e-06  9.54e-08  5.51e-07  9.61e-01  \n",
      " 17  +6.3464e+00  +6.3464e+00  4.30e-08  1.87e-09  1.27e-07  3.26e-09  1.89e-08  9.72e-01  \n",
      " 18  +6.3464e+00  +6.3464e+00  1.82e-09  7.92e-11  5.37e-09  1.37e-10  8.00e-10  9.67e-01  \n",
      "---------------------------------------------------------------------------------------------\n",
      "Terminated with status = Solved\n",
      "solve time = 5.454468304s\n",
      "-------------------------------------------------------------------------------\n",
      "                                    Summary                                    \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Sep 10 01:47:06 PM: Problem status: optimal\n",
      "(CVXPY) Sep 10 01:47:06 PM: Optimal value: 6.346e+00\n",
      "(CVXPY) Sep 10 01:47:06 PM: Compilation took 1.625e+01 seconds\n",
      "(CVXPY) Sep 10 01:47:06 PM: Solver (including time spent in interface) took 5.551e+00 seconds\n",
      "Bias (b) of SVM model is: 1.7798137266344365\n",
      "Sum of all dimensions of weight vector (w) is: -0.14521544474819548\n",
      "Resulting accuracy on validation data is: 0.9695555555555555\n",
      "Resulting accuracy on test data is: 0.968\n"
     ]
    }
   ],
   "source": [
    "# Train primal model with C=100 and then test\n",
    "svm_primal_model = svm_train_primal(X_train, y_train, 100)\n",
    "val_accuracy = svm_predict_primal(X_val, y_val, svm_primal_model)\n",
    "test_accuracy = svm_predict_primal(X_test, y_test, svm_primal_model)\n",
    "\n",
    "print(f\"Bias (b) of SVM model is: {svm_primal_model['b']}\")\n",
    "print(f\"Sum of all dimensions of weight vector (w) is: {np.sum(svm_primal_model['w'])}\")\n",
    "print(f\"Resulting accuracy on validation data is: {val_accuracy}\")\n",
    "print(f\"Resulting accuracy on test data is: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the trained SVM model parameters using dual form\n",
    "def svm_train_dual(data_train, label_train, regularisation_para_C):\n",
    "    num_samples, num_features = data_train.shape\n",
    "\n",
    "    alpha = cp.Variable(num_samples) # Optimization variable\n",
    "    # kernel = data_train @ data_train.T\n",
    "\n",
    "    # Define objective function\n",
    "    objective = cp.Maximize(cp.sum(alpha) - 0.5 * cp.sum_squares(cp.multiply(alpha, label_train) @ data_train))\n",
    "    \n",
    "    # Define constraints\n",
    "    constraints = [\n",
    "        alpha >= 0,\n",
    "        alpha <= regularisation_para_C / num_samples,\n",
    "        cp.sum(cp.multiply(alpha, label_train)) == 0\n",
    "    ]\n",
    "    \n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve(solver=cp.CVXOPT)\n",
    "\n",
    "    # Return SVM model parameter\n",
    "    svm_dual_model = {\"alpha\": alpha.value}\n",
    "    return svm_dual_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train dual model with C=100\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m svm_dual_model \u001b[38;5;241m=\u001b[39m svm_train_dual(X_train, y_train, \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum of all dimensions of optimal alpha is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39msum(svm_dual_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36msvm_train_dual\u001b[0;34m(data_train, label_train, regularisation_para_C)\u001b[0m\n\u001b[1;32m     12\u001b[0m constraints \u001b[38;5;241m=\u001b[39m [alpha \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     13\u001b[0m                alpha \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m regularisation_para_C \u001b[38;5;241m/\u001b[39m num_samples,\n\u001b[1;32m     14\u001b[0m                cp\u001b[38;5;241m.\u001b[39msum(cp\u001b[38;5;241m.\u001b[39mmultiply(alpha, label_train)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m prob \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mProblem(objective, constraints)\n\u001b[0;32m---> 17\u001b[0m prob\u001b[38;5;241m.\u001b[39msolve(solver\u001b[38;5;241m=\u001b[39mcp\u001b[38;5;241m.\u001b[39mCVXOPT)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Return SVM model parameter\u001b[39;00m\n\u001b[1;32m     20\u001b[0m svm_dual_model \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m: alpha\u001b[38;5;241m.\u001b[39mvalue}\n",
      "File \u001b[0;32m~/anaconda3/envs/isml-assignment02/lib/python3.12/site-packages/cvxpy/problems/problem.py:503\u001b[0m, in \u001b[0;36mProblem.solve\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     solve_func \u001b[38;5;241m=\u001b[39m Problem\u001b[38;5;241m.\u001b[39m_solve\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m solve_func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/isml-assignment02/lib/python3.12/site-packages/cvxpy/problems/problem.py:1086\u001b[0m, in \u001b[0;36mProblem._solve\u001b[0;34m(self, solver, warm_start, verbose, gp, qcp, requires_grad, enforce_dpp, ignore_dpp, canon_backend, **kwargs)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m solver_verbose \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m verbose):\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28mprint\u001b[39m(_NUM_SOLVER_STR)\n\u001b[0;32m-> 1086\u001b[0m solution \u001b[38;5;241m=\u001b[39m solving_chain\u001b[38;5;241m.\u001b[39msolve_via_data(\n\u001b[1;32m   1087\u001b[0m     \u001b[38;5;28mself\u001b[39m, data, warm_start, solver_verbose, kwargs)\n\u001b[1;32m   1088\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solve_time \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/isml-assignment02/lib/python3.12/site-packages/cvxpy/reductions/solvers/solving_chain.py:475\u001b[0m, in \u001b[0;36mSolvingChain.solve_via_data\u001b[0;34m(self, problem, data, warm_start, verbose, solver_opts)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msolve_via_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, problem, data, warm_start: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    440\u001b[0m                    solver_opts\u001b[38;5;241m=\u001b[39m{}):\n\u001b[1;32m    441\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Solves the problem using the data output by the an apply invocation.\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    The semantics are:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m        a Solution object.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver\u001b[38;5;241m.\u001b[39msolve_via_data(data, warm_start, verbose,\n\u001b[1;32m    476\u001b[0m                                       solver_opts, problem\u001b[38;5;241m.\u001b[39m_solver_cache)\n",
      "File \u001b[0;32m~/anaconda3/envs/isml-assignment02/lib/python3.12/site-packages/cvxpy/reductions/solvers/conic_solvers/cvxopt_conif.py:201\u001b[0m, in \u001b[0;36mCVXOPT.solve_via_data\u001b[0;34m(self, data, warm_start, verbose, solver_opts, solver_cache)\u001b[0m\n\u001b[1;32m    198\u001b[0m     kktsolver \u001b[38;5;241m=\u001b[39m kktsolver(c, G, h, dims, A, b)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     results_dict \u001b[38;5;241m=\u001b[39m cvxopt\u001b[38;5;241m.\u001b[39msolvers\u001b[38;5;241m.\u001b[39mconelp(c, G, h, dims, A, b,\n\u001b[1;32m    202\u001b[0m                                          kktsolver\u001b[38;5;241m=\u001b[39mkktsolver)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Catch exceptions in CVXOPT and convert them to solver errors.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/isml-assignment02/lib/python3.12/site-packages/cvxopt/coneprog.py:1067\u001b[0m, in \u001b[0;36mconelp\u001b[0;34m(c, G, h, dims, A, b, primalstart, dualstart, kktsolver, xnewcopy, xdot, xaxpy, xscal, ynewcopy, ydot, yaxpy, yscal, **kwargs)\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# f3(x, y, z) solves\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m#     [ 0  A'  G'   ] [ ux        ]   [ bx ]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m#     [-A   0   0     ]*[ y1        ] = -dgi * [ b ].\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;66;03m#     [-G   0   W'*W  ] [ W^{-1}*z1 ]          [ h ]\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1067\u001b[0m     f3 \u001b[38;5;241m=\u001b[39m kktsolver(W)\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m iters \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1069\u001b[0m         x1, y1 \u001b[38;5;241m=\u001b[39m xnewcopy(c), ynewcopy(b)\n",
      "File \u001b[0;32m~/anaconda3/envs/isml-assignment02/lib/python3.12/site-packages/cvxopt/coneprog.py:585\u001b[0m, in \u001b[0;36mconelp.<locals>.kktsolver\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkktsolver\u001b[39m(W):\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m factor(W)\n",
      "File \u001b[0;32m~/anaconda3/envs/isml-assignment02/lib/python3.12/site-packages/cvxopt/misc.py:1282\u001b[0m, in \u001b[0;36mkkt_chol.<locals>.factor\u001b[0;34m(W, H, Df)\u001b[0m\n\u001b[1;32m   1279\u001b[0m lapack\u001b[38;5;241m.\u001b[39mormqr(QA, tauA, K, side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m# Cholesky factorization of 2,2 block of K.\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m lapack\u001b[38;5;241m.\u001b[39mpotrf(K, n \u001b[38;5;241m=\u001b[39m n\u001b[38;5;241m-\u001b[39mp, offsetA \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m*\u001b[39m(n\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msolve\u001b[39m(x, y, z):\n\u001b[1;32m   1285\u001b[0m \n\u001b[1;32m   1286\u001b[0m     \u001b[38;5;66;03m# Solve\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;66;03m# bzp := W^{-T} * bz in packed storage \u001b[39;00m\n\u001b[1;32m   1306\u001b[0m     scale(z, W, trans \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m, inverse \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train dual model with C=100\n",
    "svm_dual_model = svm_train_dual(X_train, y_train, 100)\n",
    "\n",
    "print(f\"Sum of all dimensions of optimal alpha is: {np.sum(svm_dual_model['alpha'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To obtain primal problem solution from dual problem\n",
    "alpha = svm_dual_model['alpha']\n",
    "num_samples, num_features = X_train.shape\n",
    "\n",
    "# Calculate weight vector\n",
    "w_star = np.sum(alpha[:, None] * y_train[:, None] * X_train, axis=0)\n",
    "\n",
    "# Calculate bias term by identifying support vectors where their alpha > 0\n",
    "support_vectors = np.where(alpha > 1e-5)[0]\n",
    "bias_support_vectors = y_train[support_vectors] - np.dot(X_train[support_vectors], w_star)\n",
    "b_star = np.mean(bias_support_vectors) # Average bias values\n",
    "\n",
    "print(f\"Sum of all dimensions of weight vector for primal problem from dual solution is: {np.sum(w_star)}\")\n",
    "print(f\"Solution of bias term from primal problem from dual solution is: {b_star}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find support vectors from primal solutions\n",
    "constraint = X_train@svm_primal_model[\"w\"] + svm_primal_model[\"b\"] # Constraint of primal problem\n",
    "\n",
    "# Identify support vectors that fit the constraint\n",
    "support_vectors = (np.multiply(y_train, constraint) - 1 + svm_primal_model[\"xi\"]) <= 1e-4\n",
    "primal_support_vectors = X_train[support_vectors]\n",
    "\n",
    "print(f\"Number of support vectors found from primal solutions is: {len(primal_support_vectors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find support vectors from dual solution\n",
    "support_vectors = np.where(alpha > 1e-5)[0] # Identify support vectors that fit the dual constraint\n",
    "dual_support_vectors = X_train[support_vectors]\n",
    "\n",
    "print(f\"Number of support vectors found from dual solution is: {len(dual_support_vectors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find optimal regularisation parameter C using grid search\n",
    "def find_optimal_C(data_train, label_train, data_val, label_val, C_values):\n",
    "    optimal_C = None\n",
    "    optimal_accuracy = 0\n",
    "\n",
    "    # Train and test svm model with different values of C using primal solution\n",
    "    for regularisation_para_C in C_values:\n",
    "        svm_primal_model = svm_train_primal(data_train, label_train, regularisation_para_C)\n",
    "        val_accuracy = svm_predict_primal(data_val, label_val, svm_primal_model)\n",
    "\n",
    "        # If current validation accuracy is higher than current optimal accuracy, update\n",
    "        if val_accuracy > optimal_accuracy:\n",
    "            optimal_C = regularisation_para_C\n",
    "            optimal_accuracy = val_accuracy\n",
    "    \n",
    "    return optimal_C, optimal_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find optimal C\n",
    "C_values = [2**-10, 2**-8, 2**-6, 2**-4, 2**-2, 2**0, 2**2, 2**4, 2**8, 2**10] # 2**-10, 2**100, optimal - 0.0039...\n",
    "optimal_C, optimal_accuracy = find_optimal_C(X_train, y_train, X_val, y_val, C_values)\n",
    "\n",
    "print(f\"Optimal regularisation parameter C is: {optimal_C} with validation accuracy of {optimal_accuracy}\")\n",
    "\n",
    "# To report test accuracy using optimal C\n",
    "svm_primal_model = svm_train_primal(X_train, y_train, optimal_C)\n",
    "test_accuracy = svm_predict_primal(X_test, y_test, svm_primal_model)\n",
    "\n",
    "print(f\"Test accuracy of primal model using optimal C is: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To perform classification with linear SVM using Scikit-Learn SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Testing with penalty \"l1\" and loss \"squared_hinge\"\n",
    "model = LinearSVC(penalty='l1', loss='squared_hinge', C=optimal_C, max_iter=10000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "val_predict = model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_predict)\n",
    "\n",
    "# Predict on test set\n",
    "test_predict = model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predict)\n",
    "\n",
    "print(f\"LinearSVM model with penalty 'l1' and loss 'squared_hinge' with optimal C '{optimal_C}':\")\n",
    "print(f\"Validation accuracy: {val_accuracy} and test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with penalty \"l2\" and loss \"squared_hinge\"\n",
    "model = LinearSVC(penalty='l2', loss='squared_hinge', C=optimal_C, max_iter=10000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "val_predict = model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_predict)\n",
    "\n",
    "# Predict on test set\n",
    "test_predict = model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predict)\n",
    "\n",
    "print(f\"LinearSVM model with penalty 'l2' and loss 'squared_hinge' with optimal C '{optimal_C}':\")\n",
    "print(f\"Validation accuracy: {val_accuracy} and test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hence, final test accuracy of classification with LinearSVM model using Scikit-learn is 0.9713 or 97.13%*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
